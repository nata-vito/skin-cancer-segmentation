<div align="center">
  <h1>U-Net SKIN CANCER SEGMENTATION<h1>
  <img width="100%" src="assets/results.png" alt="Inference Result Example">
  <h3>Natanael F. G. Vitorino<h3>
  <h3>Eng. da Computa√ß√£o<h3>
  <h4>Facens<h4>
</div>


Recentemente, eu me desafiei a implementar a rede U-Net para aprender como ela funciona e desenvolvi este projeto. O objetivo desse projeto √© treinar modelos de segmenta√ß√£o com m√°scara png. Este trabalho usa o [modelo UNet] (https://arxiv.org/pdf/1505.04597) especializado em segmenta√ß√£o de imagens biom√©dicas para segmentar c√¢nceres de pele. Usei o conjunto de dados [Skin cancer: HAM10000] (https://www.kaggle.com/datasets/surajghuwalewala/ham1000-segmentation-and-classification/data), uma vers√£o de f√°cil download do [The HAM10000 dataset, uma grande cole√ß√£o de imagens dermatosc√≥picas de m√∫ltiplas fontes de les√µes cut√¢neas pigmentadas comuns] (https://doi.org/10.7910/DVN/DBW86T) e treinei o modelo. O conjunto de dados usado consiste em imagens .jpg e m√°scaras bin√°rias .png. Neste projeto, tamb√©m estou usando a sobreposi√ß√£o da m√°scara prevista com a imagem original, o que facilita a compreens√£o da √°rea afetada pelo c√¢ncer de pele. Se voc√™ estiver interessado no assunto, fique √† vontade para saber mais sobre essa arquitetura. Preparei um notebook f√°cil de entender e de implementar.

Para o pojeto de AF, gostaria de atacar os principais pontos de evolu√ß√£o deste trabalho, os quais s√£o descritos na se√ß√£o de pr√≥ximos passos deste documento. A seguir, exploro a arquitetura do modelo U-Net, apresento a analise inicial dos resultados obtido, realizo a extra√ß√£o da unica m√©trica abordada para a valida√ß√£o e apresento uma breve conclus√£o do projeto.


## U-Net Architecture

A arquitetura U-Net alcan√ßa um desempenho muito bom em aplica√ß√µes de segmenta√ß√£o em imagens biom√©dicas. Arquitetura U-net (exemplo para 32x32 pixels na resolu√ß√£o mais baixa), conforme apresentado na Figura 1. Cada caixa azul corresponde a um mapeamento de caracter√≠sticas multicanal. O n√∫mero de canais √© indicado na parte superior da caixa. O tamanho x-y √© fornecido na borda inferior esquerda da caixa. As caixas brancas representam mapas de caracter√≠sticas copiados. As setas indicam as diferentes opera√ß√µes. Este trabalho √© baseado em [Ronneberger et al] (https://arxiv.org/pdf/1505.04597).

<div align="center">
  <h3>Figure 1<h3>
  <img width="70%" src="./assets/image.png" alt="U-net architecture">
  <h4>Ronneberger et al.<h4>
</div>


## COMO INSTALAR
```sh
conda create -n torch python==3.9
conda activate torch

git clone https://github.com/nata-vito/skin-cancer-segmentation.git
cd skin-cancer-segmentation
pip install -r requirement.txt
```

Voc√™ pode encontrar o notebook em ```./notebooks/UNet_skin_cancer_seg.ipynb`` e o modelo [aqui] (https://huggingface.co/natavito/skin_cancer_seg/blob/main/README.md).

## Analise dos Dados

### M√©tricas
A segunda vers√£o do modelo melhorou em compara√ß√£o com a primeira, como pode ser visto na imagem abaixo. Os dois modelos s√£o submetidos √† mesma imagem para infer√™ncia, e a tabela a seguir mostra os resultados da infer√™ncia.

A m√©trica usada para avaliar o modelo foi a pontua√ß√£o DICE.

Pontua√ß√£o DICE = 2 * |A ‚à© B| / (|A| + |B|)

Pontua√ß√£o de dados = 2 * (n√∫mero de elementos comuns) / (n√∫mero de elementos no conjunto A + n√∫mero de elementos no conjunto B)

[Coeficiente de dados! O que √© isso?](https://medium.com/@lathashreeh/dice-coefficient-what-is-it-ff090ec97bda)


<div align="center">
  <h3>Figure 2<h3>
  <img width="50%" src="./assets/image-2.png" alt="DICE Metric">
  <h4>https://medium.com/@lathashreeh/dice-coefficient-what-is-it-ff090ec97bda<h4>
</div>

---

### Test Loss e Test DICE
| Model | Test Loss | Test DICE |
| ------------- | ------------- | ------------- |
| skin_cancer_v1.pth | 0.14458022380454671 | 0.8940358493063185 |
| skin_cancer_v2.pth | **0.13620347219208875** | **0.9024439165516506** |

---

### Resultados da Infer√™ncia

| Model | Image | DICE Coeficient |
| ------------- | ------------- | ------------- |
| skin_cancer_v1.pth | ISIC_0033463.jpg | 0.95799 |
| skin_cancer_v1.pth | ISIC_0026023.jpg | 0.79981 |
| **skin_cancer_v1.pth** | ISIC_0029811.jpg | **0.79044** |
| **skin_cancer_v1.pth** | ISIC_0029382.jpg | **0.90416** |
| skin_cancer_v1.pth | ISIC_0032059.jpg | 0.80779 |

| Model | Image | DICE Coeficient |
------------- | ------------- | ------------- |
| **skin_cancer_v2.pth** | ISIC_0033463.jpg | **0.96696** |
| **skin_cancer_v2.pth** | ISIC_0026023.jpg | **0.89177** |
| skin_cancer_v2.pth | ISIC_0029811.jpg | 0.74671 |
| skin_cancer_v2.pth | ISIC_0029382.jpg | 0.85114 |
| **skin_cancer_v2.pth** | ISIC_0032059.jpg | **0.87269** |

---

### An√°lise dos dados
Ao analisar os gr√°ficos de loss e DICE, podemos ver que, da √©poca 12 √† 20, o modelo n√£o aprendeu muito, mas houve uma pequena melhoria nos coeficientes.

<div align="center">
  <h3>Figure 3<h3>
  <img width="100%" src="./assets/results_val_loss_v1.png" alt="Validation Train Results">
  <h4>Validation Results for model version 1<h4>
</div>

<div align="center">
  <h3>Figure 4<h3>
  <img width="100%" src="./assets/results_val_loss.png" alt="Validation Train Results">
  <h4>Validation Results for model version 2<h4>
</div>

---

<div align="center">
  <h3>Model: skin_cancer_v1.pth<h3>
  <h3>ISIC_0033463.jpg<h3>
  <img width="100%" src="./assets/image-3.png" alt="DICE Metric">
  <h3>ISIC_0026023.jpg<h3>
  <img width="100%" src="./assets/image-4.png" alt="DICE Metric">
  <h3>ISIC_0029811.jpg<h3>
  <img width="100%" src="./assets/image-5.png" alt="DICE Metric">
  <h3>ISIC_0029382.jpg<h3>
  <img width="100%" src="./assets/image-6.png" alt="DICE Metric">
   <h3>ISIC_0032059.jpg<h3>
  <img width="100%" src="./assets/image-7.png" alt="DICE Metric">
</div>


---


<div align="center">
  <h3>Model: skin_cancer_v2.pth<h3>
  <h3>ISIC_0033463.jpg<h3>
  <img width="100%" src="./assets/image-8.png" alt="DICE Metric">
  <h3>ISIC_0026023.jpg<h3>
  <img width="100%" src="./assets/image-9.png" alt="DICE Metric">
  <h3>ISIC_0029811.jpg<h3>
  <img width="100%" src="./assets/image-10.png" alt="DICE Metric">
  <h3>ISIC_0029382.jpg<h3>
  <img width="100%" src="./assets/image-11.png" alt="DICE Metric">
   <h3>ISIC_0032059.jpg<h3>
  <img width="100%" src="./assets/image-12.png" alt="DICE Metric">
</div>


### Conclus√£o
Os experimentos conduzidos demonstram que a arquitetura U-Net √© uma abordagem eficaz para a segmenta√ß√£o de c√¢ncer de pele em imagens biom√©dicas. Os resultados obtidos indicam que a segunda vers√£o do modelo superou a primeira, evidenciado por uma redu√ß√£o na fun√ß√£o de perda e um aumento na pontua√ß√£o DICE. Essa melhoria est√° diretamente relacionada ao maior n√∫mero de √©pocas de treinamento, permitindo um aprendizado mais profundo das caracter√≠sticas das les√µes. No entanto, a an√°lise das curvas de loss e DICE revelou que, ap√≥s a 12¬™ √©poca, os ganhos em desempenho foram marginais, sugerindo a necessidade de otimiza√ß√µes mais avan√ßadas no treinamento do modelo.


### Pr√≥ximos Passos
Para aprimorar o desempenho do modelo de segmenta√ß√£o, faz-se necess√°rio um refinamento no processo de fine-tuning. Isso inclui a experimenta√ß√£o com diferentes hiperpar√¢metros, como taxa de aprendizado, otimizadores (Adam, RMSprop, SGD) e fun√ß√µes de perda adequadas para segmenta√ß√£o. Al√©m disso, ser√° analisado o impacto do aumento do n√∫mero de √©pocas no treinamento, buscando um equil√≠brio entre a melhoria da acur√°cia e a mitiga√ß√£o do overfitting. T√©cnicas como early stopping e learning rate scheduling ser√£o empregadas para otimizar o processo de aprendizado e garantir a melhor converg√™ncia do modelo dentro de um tempo computacional razo√°vel.


Outra etapa fundamental consiste na avalia√ß√£o comparativa de diferentes arquiteturas de segmenta√ß√£o. Al√©m da U-Net, ser√£o testadas redes como DeepLabV3+, FPN (Feature Pyramid Network) e SegFormer, que apresentam avan√ßos na extra√ß√£o de caracter√≠sticas espaciais e contexto sem√¢ntico. Cada uma dessas arquiteturas ser√° treinada sob condi√ß√µes semelhantes para possibilitar uma compara√ß√£o justa, e seus desempenhos ser√£o analisados com base em m√©tricas padronizadas. Essa an√°lise permitir√° a identifica√ß√£o do modelo mais eficiente para a tarefa de segmenta√ß√£o de c√¢ncer de pele, levando em considera√ß√£o tanto a precis√£o quanto a capacidade de generaliza√ß√£o.


Por fim, ser√° conduzida uma avalia√ß√£o mais abrangente da qualidade das segmenta√ß√µes por meio da incorpora√ß√£o de m√©tricas adicionais, como Intersection over Union (IoU), Sensibilidade, Especificidade e Matriz de Confus√£o. O uso dessas m√©tricas fornecer√° uma vis√£o mais detalhada do comportamento do modelo, permitindo identificar poss√≠veis vieses e limita√ß√µes. Al√©m disso, ser√£o aprimoradas as t√©cnicas de visualiza√ß√£o dos resultados, incluindo gr√°ficos mais detalhados sobre a evolu√ß√£o do treinamento e a sobreposi√ß√£o das m√°scaras preditas nas imagens originais. Com essas melhorias, espera-se obter um modelo mais robusto e confi√°vel para aplica√ß√µes pr√°ticas na detec√ß√£o e segmenta√ß√£o de c√¢ncer de pele.


## ü§ù Autor

Gostar√≠amos de agradecer √†s seguintes pessoas que contribu√≠ram para este projeto:

<table>
  <tr>
    <td align="center">
      <a href="#">
        <img src="https://avatars.githubusercontent.com/u/64169072?v=4" width="100px;" alt="Foto do Natanael Vitorino no GitHub"/><br>
        <sub>
          <b>Natanael Vitorino</b>
        </sub>
      </a>
    </td>
  </tr>
</table>


## üìù License

Este projeto est√° sob licen√ßa. Consulte o arquivo [LICENSE](LICENSE) para obter mais detalhes.

---
